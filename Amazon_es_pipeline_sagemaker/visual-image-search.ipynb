{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (4.62.3)\n",
      "Requirement already satisfied: elasticsearch in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (8.1.2)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from elasticsearch) (8.1.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from elastic-transport<9,>=8->elasticsearch) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<2,>=1.26.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from elastic-transport<9,>=8->elasticsearch) (1.26.8)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from elastic-transport<9,>=8->elasticsearch) (0.8)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests) (1.26.8)\n",
      "Requirement already satisfied: requests-aws4auth in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (1.1.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests-aws4auth) (1.15.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests-aws4auth) (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->requests-aws4auth) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->requests-aws4auth) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->requests-aws4auth) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->requests-aws4auth) (3.1)\n"
     ]
    }
   ],
   "source": [
    "#Install tqdm to have progress bar\n",
    "!pip install tqdm\n",
    "\n",
    "#install necessary pkg to make connection with elasticsearch domain\n",
    "!pip install elasticsearch\n",
    "!pip install requests\n",
    "!pip install requests-aws4auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'esDomainName': 'vis-sea-domain-1bnj0v65kn5zg',\n",
       " 'S3BucketSecureURL': 'https://vis-search-s3buckethosting-1st2s59fkit8r.s3.amazonaws.com',\n",
       " 'esHostName': 'search-vis-sea-domain-1bnj0v65kn5zg-ns2yo2ik73rc2m5nmdi2w4mqoq.us-east-1.es.amazonaws.com',\n",
       " 'SageMakerNotebookURL': 'https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances/openNotebook/NotebookInstance-2nztVLyFBlPm?view=classic',\n",
       " 's3BucketTraining': 'vis-search-s3buckettraining-1b4wzo2ek4wo7',\n",
       " 's3BucketHostingBucketName': 'vis-search-s3buckethosting-1st2s59fkit8r'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"vis-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "\n",
    "bucket = outputs['s3BucketTraining']\n",
    "es_host = outputs['esHostName']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preparation\n",
    "\n",
    "import os \n",
    "import shutil\n",
    "import json\n",
    "import tqdm\n",
    "import urllib.request\n",
    "from tqdm import notebook\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "images_path = 'data/feidegger/fashion'\n",
    "filename = 'metadata.json'\n",
    "\n",
    "my_bucket = s3_resource.Bucket(bucket)\n",
    "\n",
    "if not os.path.isdir(images_path):\n",
    "    os.makedirs(images_path)\n",
    "\n",
    "def download_metadata(url):\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        \n",
    "#download metadata.json to local notebook\n",
    "#download_metadata('https://raw.githubusercontent.com/zalandoresearch/feidegger/master/data/FEIDEGGER_release_1.1.json')\n",
    "#download_metadata('https://raw.githubusercontent.com/zalandoresearch/feidegger/master/metadata.json')\n",
    "\n",
    "def generate_image_list(filename):\n",
    "    metadata = open(filename,'r')\n",
    "    data = json.load(metadata)\n",
    "    url_lst = []\n",
    "    for i in range(len(data)):\n",
    "        url_lst.append(data[i]['url'])\n",
    "    return url_lst\n",
    "\n",
    "\n",
    "def download_image(url):\n",
    "    urllib.request.urlretrieve(url, images_path + '/' + url.split(\"/\")[-1])\n",
    "                    \n",
    "#generate image list            \n",
    "url_lst = generate_image_list(filename)     \n",
    "\n",
    "workers = 2 * cpu_count()\n",
    "\n",
    "#downloading images to local disk\n",
    "process_map(download_image, url_lst, max_workers=workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading dataset to S3\n",
    "\n",
    "files_to_upload = []\n",
    "dirName = 'data'\n",
    "for path, subdirs, files in os.walk('./' + dirName):\n",
    "    path = path.replace(\"\\\\\",\"/\")\n",
    "    directory_name = path.replace('./',\"\")\n",
    "    for file in files:\n",
    "        files_to_upload.append({\n",
    "            \"filename\": os.path.join(path, file),\n",
    "            \"key\": directory_name+'/'+file\n",
    "        })\n",
    "        \n",
    "\n",
    "def upload_to_s3(file):\n",
    "        my_bucket.upload_file(file['filename'], file['key'])\n",
    "        \n",
    "#uploading images to s3\n",
    "process_map(upload_to_s3, files_to_upload, max_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Model Preparation\n",
    "\n",
    "We'll use TensorFlow backend to prepare a model for \"featurizing\" images into feature vectors. TensorFlow has a native Module API, as well as a higher level Keras API. \n",
    "\n",
    "We will start with a pretrained model, avoiding spending time and money training a model from scratch. Accordingly, as a first step in preparing the model, we'll import a pretrained model from Keras application. Researchers have experimented with various pretrained CNN architectures with different numbers of layers, discovering that there are several good possibilities.\n",
    "\n",
    "In this notebook, we'll select a model based on the ResNet architecture, a commonly used choice. Of the various choices for number of layers, ranging from 18 to 152, we'll use 50 layers. This also is a common choice that balances the expressiveness of the resulting feature vectors (embeddings) against computational efficiency (lower number of layers means greater efficiency at the cost of less expressiveness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "import sagemaker\n",
    "from PIL import Image\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_first\n"
     ]
    }
   ],
   "source": [
    "# Set the channel first for better performance\n",
    "from tensorflow.keras import backend\n",
    "backend.set_image_data_format('channels_first')\n",
    "print(backend.image_data_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get a reference ResNet50 model which is trained on Imagenet dataset to extract the feature without the actual clssifier. More specifically, we'll use that layer to generate a row vector of floating point numbers as an \"embedding\" or representation of the features of the image. We'll also save the model as *SavedModel* format under **export/Servo/1** to serve from SageMaker TensorFlow serving API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 1s 0us/step\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
     ]
    }
   ],
   "source": [
    "#Import Resnet50 model\n",
    "model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False,input_shape=(3, 224, 224),pooling='avg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3, 224, 224) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 3, 230, 230)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 64, 112, 112) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 64, 112, 112) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 64, 112, 112) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 64, 114, 114) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 64, 56, 56)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 64, 56, 56)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 64, 56, 56)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 64, 56, 56)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 64, 56, 56)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 64, 56, 56)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 64, 56, 56)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 256, 56, 56)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 256, 56, 56)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 256, 56, 56)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 256, 56, 56)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 256, 56, 56)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 256, 56, 56)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 64, 56, 56)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 64, 56, 56)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 64, 56, 56)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 64, 56, 56)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 64, 56, 56)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 64, 56, 56)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 256, 56, 56)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 256, 56, 56)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 256, 56, 56)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 256, 56, 56)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 64, 56, 56)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 64, 56, 56)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 64, 56, 56)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 64, 56, 56)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 64, 56, 56)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 64, 56, 56)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 256, 56, 56)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 256, 56, 56)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 256, 56, 56)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 256, 56, 56)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 128, 28, 28)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 128, 28, 28)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 128, 28, 28)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 128, 28, 28)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 128, 28, 28)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 128, 28, 28)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 512, 28, 28)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 512, 28, 28)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 512, 28, 28)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 512, 28, 28)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 512, 28, 28)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 512, 28, 28)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 128, 28, 28)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 128, 28, 28)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 128, 28, 28)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 128, 28, 28)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 128, 28, 28)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 128, 28, 28)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 512, 28, 28)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 512, 28, 28)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 512, 28, 28)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 512, 28, 28)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 128, 28, 28)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 128, 28, 28)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 128, 28, 28)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 128, 28, 28)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 128, 28, 28)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 128, 28, 28)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 512, 28, 28)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 512, 28, 28)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 512, 28, 28)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 512, 28, 28)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 128, 28, 28)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 128, 28, 28)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 128, 28, 28)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 128, 28, 28)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 128, 28, 28)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 128, 28, 28)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 512, 28, 28)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 512, 28, 28)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 512, 28, 28)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 512, 28, 28)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 256, 14, 14)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 256, 14, 14)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 256, 14, 14)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 256, 14, 14)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 256, 14, 14)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 256, 14, 14)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 1024, 14, 14) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 1024, 14, 14) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 1024, 14, 14) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 1024, 14, 14) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 1024, 14, 14) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 1024, 14, 14) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 256, 14, 14)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 256, 14, 14)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 256, 14, 14)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 256, 14, 14)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 256, 14, 14)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 256, 14, 14)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 1024, 14, 14) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 1024, 14, 14) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 1024, 14, 14) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 1024, 14, 14) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 256, 14, 14)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 256, 14, 14)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 256, 14, 14)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 256, 14, 14)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 256, 14, 14)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 256, 14, 14)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 1024, 14, 14) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 1024, 14, 14) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 1024, 14, 14) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 1024, 14, 14) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 256, 14, 14)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 256, 14, 14)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 256, 14, 14)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 256, 14, 14)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 256, 14, 14)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 256, 14, 14)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 1024, 14, 14) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 1024, 14, 14) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 1024, 14, 14) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 1024, 14, 14) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 256, 14, 14)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 256, 14, 14)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 256, 14, 14)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 256, 14, 14)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 256, 14, 14)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 256, 14, 14)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 1024, 14, 14) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 1024, 14, 14) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 1024, 14, 14) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 1024, 14, 14) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 256, 14, 14)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 256, 14, 14)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 256, 14, 14)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 256, 14, 14)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 256, 14, 14)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 256, 14, 14)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 1024, 14, 14) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 1024, 14, 14) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 1024, 14, 14) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 1024, 14, 14) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 512, 7, 7)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 512, 7, 7)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 512, 7, 7)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 512, 7, 7)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 512, 7, 7)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 512, 7, 7)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2048, 7, 7)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 2048, 7, 7)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2048, 7, 7)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 2048, 7, 7)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2048, 7, 7)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 2048, 7, 7)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 512, 7, 7)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 512, 7, 7)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 512, 7, 7)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 512, 7, 7)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 512, 7, 7)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 512, 7, 7)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 2048, 7, 7)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 2048, 7, 7)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2048, 7, 7)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 2048, 7, 7)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 512, 7, 7)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 512, 7, 7)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 512, 7, 7)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 512, 7, 7)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 512, 7, 7)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 512, 7, 7)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 2048, 7, 7)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 2048, 7, 7)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 2048, 7, 7)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 2048, 7, 7)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  export/Servo/1  Created \n"
     ]
    }
   ],
   "source": [
    "#Creating the directory strcture\n",
    "dirName = 'export/Servo/1'\n",
    "if not os.path.exists(dirName):\n",
    "    os.makedirs(dirName)\n",
    "    print(\"Directory \" , dirName ,  \" Created \")\n",
    "else:    \n",
    "    print(\"Directory \" , dirName ,  \" already exists\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./export/Servo/1/assets\n"
     ]
    }
   ],
   "source": [
    "#Save the model in SavedModel format\n",
    "model.save('./export/Servo/1/', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: saved_model_cli: command not found\r\n"
     ]
    }
   ],
   "source": [
    "#Check the model Signature\n",
    "!saved_model_cli show --dir ./export/Servo/1/ --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model Hosting\n",
    "\n",
    "After saving the feature extractor model we will deploy the model using Sagemaker Tensorflow Serving api which is a flexible, high-performance serving system for machine learning models, designed for production environments.TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data. We will define **inference.py** to customize the input data to TensorFlow serving API. We also need to add **requirements.txt** file for aditional libraby in the tensorflow serving container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mbase64\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m namedtuple\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpreprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m image\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mapplications\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mresnet50\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ResNet50, preprocess_input\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Image\r\n",
      "\r\n",
      "\u001b[37m# restricting memory growth\u001b[39;49;00m\r\n",
      "physical_gpus = tf.config.experimental.list_physical_devices(\u001b[33m'\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\u001b[34mif\u001b[39;49;00m physical_gpus:\r\n",
      "  \u001b[34mtry\u001b[39;49;00m:\r\n",
      "    \u001b[37m# Currently, memory growth needs to be the same across GPUs\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m gpu \u001b[35min\u001b[39;49;00m physical_gpus:\r\n",
      "      tf.config.experimental.set_memory_growth(gpu, \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    logical_gpus = tf.config.experimental.list_logical_devices(\u001b[33m'\u001b[39;49;00m\u001b[33mGPU\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(physical_gpus), \u001b[33m\"\u001b[39;49;00m\u001b[33mPhysical GPUs,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mlen\u001b[39;49;00m(logical_gpus), \u001b[33m\"\u001b[39;49;00m\u001b[33mLogical GPUs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "  \u001b[34mexcept\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\r\n",
      "    \u001b[37m# Memory growth must be set before GPUs have been initialized\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(e)\r\n",
      "\r\n",
      "HEIGHT = \u001b[34m224\u001b[39;49;00m\r\n",
      "WIDTH  = \u001b[34m224\u001b[39;49;00m\r\n",
      "\r\n",
      "Context = namedtuple(\u001b[33m'\u001b[39;49;00m\u001b[33mContext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                     \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_name, model_version, method, rest_uri, grpc_uri, \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                     \u001b[33m'\u001b[39;49;00m\u001b[33mcustom_attributes, request_content_type, accept_header\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_handler\u001b[39;49;00m(data, context):\r\n",
      "    \u001b[33m\"\"\" Pre-process request input before it is sent to TensorFlow Serving REST API\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m        data (obj): the request data, in format of dict or string\u001b[39;49;00m\r\n",
      "\u001b[33m        context (Context): an object containing request and configuration details\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m        (dict): a JSON-serializable dict that contains request body and headers\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m context.request_content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-image\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        \u001b[37m# pass through json (assumes it's correctly formed)\u001b[39;49;00m\r\n",
      "        \u001b[37m#read image as bytes\u001b[39;49;00m\r\n",
      "        image_as_bytes = io.BytesIO(data.read())\r\n",
      "        img = Image.open(image_as_bytes)\r\n",
      "        img = img.resize((WIDTH, HEIGHT))\r\n",
      "        \u001b[37m# convert PIL image instance to numpy array\u001b[39;49;00m\r\n",
      "        img_array = image.img_to_array(img, data_format = \u001b[33m\"\u001b[39;49;00m\u001b[33mchannels_first\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[37m# the image is now in an array of shape (3, 224, 224)\u001b[39;49;00m\r\n",
      "        \u001b[37m# need to expand it to (1, 3, 224, 224) as it's expecting a list\u001b[39;49;00m\r\n",
      "        expanded_img_array = tf.expand_dims(img_array, axis=\u001b[34m0\u001b[39;49;00m)\r\n",
      "        \u001b[37m#preprocessing the image array with channel first\u001b[39;49;00m\r\n",
      "        preprocessed_img = preprocess_input(expanded_img_array, data_format = \u001b[33m\"\u001b[39;49;00m\u001b[33mchannels_first\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[37m#converting to numpy list\u001b[39;49;00m\r\n",
      "        preprocessed_img_lst = preprocessed_img.numpy().tolist()\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m json.dumps({\u001b[33m\"\u001b[39;49;00m\u001b[33minstances\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: preprocessed_img_lst})\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        _return_error(\u001b[34m415\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mUnsupported content type \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(context.request_content_type \u001b[35mor\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mUnknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_handler\u001b[39;49;00m(data, context):\r\n",
      "    \u001b[33m\"\"\"Post-process TensorFlow Serving output before it is returned to the client.\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m        data (obj): the TensorFlow serving response\u001b[39;49;00m\r\n",
      "\u001b[33m        context (Context): an object containing request and configuration details\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m        (bytes, string): data to return to client, response content type\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m data.status_code != \u001b[34m200\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(data.content.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "    response_content_type = context.accept_header\r\n",
      "    prediction = data.content\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m prediction,response_content_type\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_return_error\u001b[39;49;00m(code, message):\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mError: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mstr\u001b[39;49;00m(code), message))\r\n"
     ]
    }
   ],
   "source": [
    "#check the actual content of inference.py\n",
    "!pygmentize src/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "#zip the model .gz format\n",
    "model_version = '1'\n",
    "export_dir = 'export/Servo/' + model_version\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('export', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-896542110133/model/model.tar.gz'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Upload the model to S3\n",
    "sagemaker_session = sagemaker.Session()\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we upload the model to S3 we will use TensorFlow serving container to host the model. We are using ml.p3.16xlarge instance type. You may need to raise support ticket to increase the Service quotas for SageMaker hosting instance type. We will use this endpoint to generate features and import into ElasticSearch. you can also choose small instance such as \"ml.m4.xlarge\" to save cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.tensorflow.serving.Model has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "#Deploy the model in Sagemaker Endpoint. This process will take ~10 min.\n",
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "sagemaker_model = Model(entry_point='inference.py', model_data = 's3://' + sagemaker_session.default_bucket() + '/model/model.tar.gz',\n",
    "                                  role = role, framework_version='2.1.0', source_dir='./src' )\n",
    "\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=3, instance_type='ml.m5.xlarge')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchKey",
     "evalue": "An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchKey\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-3b7a2f6a6c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get the features for a sample image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIdentitySerializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/feidegger/fashion/0VB21C000-A11@12.1.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIdentitySerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'application/x-image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    400\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchKey\u001b[0m: An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist."
     ]
    }
   ],
   "source": [
    "# get the features for a sample image\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "payload = s3.get_object(Bucket=bucket,Key='data/feidegger/fashion/0VB21C000-A11@12.1.jpg')['Body'].read()\n",
    "predictor.serializer = IdentitySerializer(content_type='application/x-image')\n",
    "features = predictor.predict(payload)['predictions'][0]\n",
    "\n",
    "features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a KNN Index in Elasticsearch\n",
    "\n",
    "KNN for Amazon Elasticsearch Service lets you search for points in a vector space and find the \"nearest neighbors\" for those points by Euclidean distance or cosine similarity (default is Euclidean distance). Use cases include recommendations (for example, an \"other songs you might like\" feature in a music application), image recognition, and fraud detection.\n",
    "\n",
    "KNN requires Elasticsearch 7.1 or later. Full documentation for the Elasticsearch feature, including descriptions of settings and statistics, is available in the Open Distro for Elasticsearch documentation. For background information about the k-nearest neighbors algorithm\n",
    "\n",
    "In this step we'll get all the features zalando images and import those features into Elastichseach7.4 domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some utility function\n",
    "\n",
    "#return all s3 keys\n",
    "def get_all_s3_keys(bucket):\n",
    "    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"    \n",
    "    keys = []\n",
    "\n",
    "    kwargs = {'Bucket': bucket}\n",
    "    while True:\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp['Contents']:\n",
    "            keys.append('s3://' + bucket + '/' + obj['Key'])\n",
    "\n",
    "        try:\n",
    "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the zalando images keys from the bucket make a list\n",
    "s3_uris = get_all_s3_keys(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# define a function to extract image features\n",
    "from time import sleep\n",
    "\n",
    "sm_client = boto3.client('sagemaker-runtime')\n",
    "ENDPOINT_NAME = predictor.endpoint\n",
    "\n",
    "def get_predictions(payload):\n",
    "    return sm_client.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                           ContentType='application/x-image',\n",
    "                                           Body=payload)\n",
    "\n",
    "def extract_features(s3_uri):\n",
    "    key = s3_uri.replace(f's3://{bucket}/', '')\n",
    "    payload = s3.get_object(Bucket=bucket,Key=key)['Body'].read()\n",
    "    try:\n",
    "        response = get_predictions(payload)\n",
    "    except:\n",
    "        sleep(0.1)\n",
    "        response = get_predictions(payload)\n",
    "\n",
    "    del payload\n",
    "    response_body = json.loads((response['Body'].read()))\n",
    "    feature_lst = response_body['predictions'][0]\n",
    "    \n",
    "    return s3_uri, feature_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:7: TqdmWarning: Iterable length 8792 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68cb831297848b89c33781d3099f1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This process cell will take approximately 24-25 minutes on a t3.medium notebook instance\n",
    "# with 3 m5.xlarge SageMaker Hosted Endpoint instances\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "workers = 2 * cpu_count()\n",
    "result = process_map(extract_features, s3_uris, max_workers=workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RequestsHttpConnection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-677e6d28483c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# setting up the Elasticsearch connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0melasticsearch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mElasticsearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequestsHttpConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrequests_aws4auth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAWS4Auth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mregion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'us-east-1'\u001b[0m \u001b[0;31m# e.g. us-east-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'es'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RequestsHttpConnection'"
     ]
    }
   ],
   "source": [
    "# setting up the Elasticsearch connection\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "region = 'us-east-1' # e.g. us-east-1\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts = [{'host': es_host, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define KNN Elasticsearch index maping\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"zalando_img_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 2048\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'es' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-5af2221c4729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Creating the Elasticsearch index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"idx_zalando\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknn_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"idx_zalando\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'es' is not defined"
     ]
    }
   ],
   "source": [
    "#Creating the Elasticsearch index\n",
    "es.indices.create(index=\"idx_zalando\",body=knn_index,ignore=400)\n",
    "es.indices.get(index=\"idx_zalando\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:11: TqdmWarning: Iterable length 8792 > 1000 but `chunksize` is not set. This may seriously degrade multiprocess performance. Set `chunksize=1` or more.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acea14be01644a9a376e60ae5766a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'es' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/process.py\", line 175, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/process.py\", line 153, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/process.py\", line 153, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"<ipython-input-43-426379fb36ea>\", line 6, in es_import\n    es.index(index='idx_zalando',\nNameError: name 'es' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-426379fb36ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprocess_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tqdm/contrib/concurrent.py\u001b[0m in \u001b[0;36mprocess_map\u001b[0;34m(fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mtqdm_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mtqdm_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lock_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mp_lock\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_executor_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtqdm_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tqdm/contrib/concurrent.py\u001b[0m in \u001b[0;36m_executor_map\u001b[0;34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mmap_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpool_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmap_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mcareful\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mto\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0mto\u001b[0m \u001b[0myielded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \"\"\"\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_process_worker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcall_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcall_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ExceptionWithTraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_process_chunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \"\"\"\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_process_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/concurrent/futures/process.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \"\"\"\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_process_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-426379fb36ea>\u001b[0m in \u001b[0;36mes_import\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mes_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     es.index(index='idx_zalando',\n\u001b[0m\u001b[1;32m      7\u001b[0m              body={\"zalando_img_vector\": i[1], \n\u001b[1;32m      8\u001b[0m                    \"image\": i[0]}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'es' is not defined"
     ]
    }
   ],
   "source": [
    "# defining a function to import the feature vectors corrosponds to each S3 URI into Elasticsearch KNN index\n",
    "# This process will take around ~3 min.\n",
    "\n",
    "\n",
    "def es_import(i):\n",
    "    es.index(index='idx_zalando',\n",
    "             body={\"zalando_img_vector\": i[1], \n",
    "                   \"image\": i[0]}\n",
    "            )\n",
    "    \n",
    "process_map(es_import, result, max_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Index Search Results\n",
    "\n",
    "In this step we will use SageMaker SDK as well as Boto3 SDK to query the Elasticsearch to retrive the nearest neighbours. One thing to mention **zalando** dataset has pretty good similarity with Imagenet dataset. Now if you hav a very domain speific problem then then you need to train that dataset on top of pretrained feature extractor model such as VGG, Resnet, Xeception, Mobilenet etc and bulid a new feature extractor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define display_image function\n",
    "def display_image(bucket, key):\n",
    "    response = s3.get_object(Bucket=bucket,Key=key)['Body']\n",
    "    img = Image.open(response)\n",
    "    return display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x7fb44b65c9e8>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-843c4cdbc9de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mimg_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mquery_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mquery_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3029\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3030\u001b[0m     raise UnidentifiedImageError(\n\u001b[0;32m-> 3031\u001b[0;31m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3032\u001b[0m     )\n\u001b[1;32m   3033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7fb44b65c9e8>"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "from PIL import Image\n",
    "import io\n",
    "urls = []\n",
    "# yellow pattern dess\n",
    "urls.append('https://fastly.hautelookcdn.com/products/D7242MNR/large/13494318.jpg')\n",
    "# T shirt kind dress\n",
    "urls.append('https://fastly.hautelookcdn.com/products/M2241/large/15658772.jpg')\n",
    "#Dotted pattern dress\n",
    "urls.append('https://fastly.hautelookcdn.com/products/19463M/large/14537545.jpg')\n",
    "\n",
    "img_bytes = requests.get(random.choice(urls)).content\n",
    "query_img = Image.open(io.BytesIO(img_bytes))\n",
    "query_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SageMaker SDK Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"{\"error\": \"cannot identify image file <_io.BytesIO object at 0x7f81b15b0f68>\"}\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/tensorflow-inference-2022-04-02-05-06-50-241 in account 896542110133 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-90be349f103a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIdentitySerializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIdentitySerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'application/x-image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/tensorflow/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CustomAttributes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorFlowPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    400\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"{\"error\": \"cannot identify image file <_io.BytesIO object at 0x7f81b15b0f68>\"}\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/tensorflow-inference-2022-04-02-05-06-50-241 in account 896542110133 for more information."
     ]
    }
   ],
   "source": [
    "#SageMaker SDK approach\n",
    "from sagemaker.serializers import IdentitySerializer\n",
    "predictor.serializer = IdentitySerializer(content_type='application/x-image')\n",
    "features = predictor.predict(img_bytes)['predictions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "k = 5\n",
    "idx_name = 'idx_zalando'\n",
    "res = es.search(request_timeout=30, index=idx_name,\n",
    "                body={'size': k, \n",
    "                      'query': {'knn': {'zalando_img_vector': {'vector': features, 'k': k}}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    key = res['hits']['hits'][i]['_source']['image']\n",
    "    key = key.replace(f's3://{bucket}/','')\n",
    "    img = display_image(bucket,key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boto3 Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker-runtime')\n",
    "ENDPOINT_NAME = predictor.endpoint # our endpoint name\n",
    "response = client.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                       ContentType='application/x-image',\n",
    "                                       Body=img_bytes)\n",
    "\n",
    "response_body = json.loads((response['Body'].read()))\n",
    "features = response_body['predictions'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "k = 5\n",
    "idx_name = 'idx_zalando'\n",
    "res = es.search(request_timeout=30, index=idx_name,\n",
    "                body={'size': k, \n",
    "                      'query': {'knn': {'zalando_img_vector': {'vector': features, 'k': k}}}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    key = res['hits']['hits'][i]['_source']['image']\n",
    "    key = key.replace(f's3://{bucket}/','')\n",
    "    img = display_image (bucket,key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a full-stack visual search application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource.Object(bucket, 'backend/template.yaml').upload_file('./backend/template.yaml', ExtraArgs={'ACL':'public-read'})\n",
    "\n",
    "\n",
    "sam_template_url = f'https://{bucket}.s3.amazonaws.com/backend/template.yaml'\n",
    "\n",
    "# Generate the CloudFormation Quick Create Link\n",
    "\n",
    "print(\"Click the URL below to create the backend API for visual search:\\n\")\n",
    "print((\n",
    "    'https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/create/review'\n",
    "    f'?templateURL={sam_template_url}'\n",
    "    '&stackName=vis-search-api'\n",
    "    f'&param_BucketName={outputs[\"s3BucketTraining\"]}'\n",
    "    f'&param_DomainName={outputs[\"esDomainName\"]}'\n",
    "    f'&param_ElasticSearchURL={outputs[\"esHostName\"]}'\n",
    "    f'&param_SagemakerEndpoint={predictor.endpoint}'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a working Amazon SageMaker endpoint for extracting image features and a KNN index on Elasticsearch, you are ready to build a real-world full-stack ML-powered web app. The SAM template you just created will deploy an Amazon API Gateway and AWS Lambda function. The Lambda function runs your code in response to HTTP requests that are sent to the API Gateway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mbase64\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m environ\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrequests\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36murllib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m urlparse\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BytesIO\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36melasticsearch\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Elasticsearch, RequestsHttpConnection\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrequests_aws4auth\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AWS4Auth\r\n",
      "\r\n",
      "\u001b[37m# Global variables that are reused\u001b[39;49;00m\r\n",
      "sm_runtime_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-runtime\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "s3_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_features\u001b[39;49;00m(sm_runtime_client, sagemaker_endpoint, img_bytes):\r\n",
      "    response = sm_runtime_client.invoke_endpoint(\r\n",
      "        EndpointName=sagemaker_endpoint,\r\n",
      "        ContentType=\u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-image\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "        Body=img_bytes)\r\n",
      "    response_body = json.loads((response[\u001b[33m'\u001b[39;49;00m\u001b[33mBody\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].read()))\r\n",
      "    features = response_body[\u001b[33m'\u001b[39;49;00m\u001b[33mpredictions\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m features\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_neighbors\u001b[39;49;00m(features, es, k_neighbors=\u001b[34m3\u001b[39;49;00m):\r\n",
      "    idx_name = \u001b[33m'\u001b[39;49;00m\u001b[33midx_zalando\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    res = es.search(\r\n",
      "        request_timeout=\u001b[34m30\u001b[39;49;00m, index=idx_name,\r\n",
      "        body={\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33msize\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: k_neighbors,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mquery\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: {\u001b[33m'\u001b[39;49;00m\u001b[33mknn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: {\u001b[33m'\u001b[39;49;00m\u001b[33mzalando_img_vector\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: {\u001b[33m'\u001b[39;49;00m\u001b[33mvector\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: features, \u001b[33m'\u001b[39;49;00m\u001b[33mk\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: k_neighbors}}}}\r\n",
      "        )\r\n",
      "    s3_uris = [res[\u001b[33m'\u001b[39;49;00m\u001b[33mhits\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mhits\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][x][\u001b[33m'\u001b[39;49;00m\u001b[33m_source\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(k_neighbors)]\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m s3_uris\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mgenerate_presigned_urls\u001b[39;49;00m(s3_uris):\r\n",
      "    presigned_urls = [s3_client.generate_presigned_url(\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mget_object\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "        Params={\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mBucket\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: urlparse(x).netloc,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mKey\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: urlparse(x).path.lstrip(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)},\r\n",
      "        ExpiresIn=\u001b[34m300\u001b[39;49;00m\r\n",
      "    ) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m s3_uris]\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m presigned_urls\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mdownload_file\u001b[39;49;00m(url):\r\n",
      "    r = requests.get(url)\r\n",
      "    \u001b[34mif\u001b[39;49;00m r.status_code == \u001b[34m200\u001b[39;49;00m:\r\n",
      "        file = BytesIO(r.content)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfile failed to download\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m file\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlambda_handler\u001b[39;49;00m(event, context):\r\n",
      "\r\n",
      "    \u001b[37m# elasticsearch variables\u001b[39;49;00m\r\n",
      "    service = \u001b[33m'\u001b[39;49;00m\u001b[33mes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    region = environ[\u001b[33m'\u001b[39;49;00m\u001b[33mAWS_REGION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    elasticsearch_endpoint = environ[\u001b[33m'\u001b[39;49;00m\u001b[33mES_ENDPOINT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    session = boto3.session.Session()\r\n",
      "    credentials = session.get_credentials()\r\n",
      "    awsauth = AWS4Auth(\r\n",
      "        credentials.access_key,\r\n",
      "        credentials.secret_key,\r\n",
      "        region,\r\n",
      "        service,\r\n",
      "        session_token=credentials.token\r\n",
      "        )\r\n",
      "\r\n",
      "    es = Elasticsearch(\r\n",
      "        hosts=[{\u001b[33m'\u001b[39;49;00m\u001b[33mhost\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: elasticsearch_endpoint, \u001b[33m'\u001b[39;49;00m\u001b[33mport\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34m443\u001b[39;49;00m}],\r\n",
      "        http_auth=awsauth,\r\n",
      "        use_ssl=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "        verify_certs=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "        connection_class=RequestsHttpConnection\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[37m# sagemaker variables\u001b[39;49;00m\r\n",
      "    sagemaker_endpoint = environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_ENDPOINT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    api_payload = json.loads(event[\u001b[33m'\u001b[39;49;00m\u001b[33mbody\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    k = api_payload[\u001b[33m'\u001b[39;49;00m\u001b[33mk\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[34mif\u001b[39;49;00m event[\u001b[33m'\u001b[39;49;00m\u001b[33mpath\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[33m'\u001b[39;49;00m\u001b[33m/postURL\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        image = download_file(api_payload[\u001b[33m'\u001b[39;49;00m\u001b[33murl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        img_string = api_payload[\u001b[33m'\u001b[39;49;00m\u001b[33mbase64img\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(img_string)\r\n",
      "        image = BytesIO(base64.b64decode(img_string))\r\n",
      "\r\n",
      "    features = get_features(sm_runtime_client, sagemaker_endpoint, image)\r\n",
      "    s3_uris_neighbors = get_neighbors(features, es, k_neighbors=k)\r\n",
      "    s3_presigned_urls = generate_presigned_urls(s3_uris_neighbors)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mstatusCode\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m200\u001b[39;49;00m,\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mheaders\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mAccess-Control-Allow-Origin\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:  \u001b[33m\"\u001b[39;49;00m\u001b[33m*\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mAccess-Control-Allow-Headers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m*\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mAccess-Control-Allow-Methods\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m*\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        },\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mbody\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: json.dumps({\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mimages\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: s3_presigned_urls,\r\n",
      "        }),\r\n",
      "    }\r\n"
     ]
    }
   ],
   "source": [
    "# Review the content of the Lambda function code.\n",
    "!pygmentize backend/lambda/app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once the CloudFormation Stack shows CREATE_COMPLETE, proceed to this cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the REST endpoint for the search API to a config file, to be used by the frontend build\n",
    "\n",
    "import json\n",
    "api_endpoint = get_cfn_outputs('vis-search-api')['ImageSimilarityApi']\n",
    "\n",
    "with open('./frontend/src/config/config.json', 'w') as outfile:\n",
    "    json.dump({'apiEndpoint': api_endpoint}, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Deploy frontend services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda-10.0/bin:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/home/ec2-user/anaconda3/condabin:/home/ec2-user/.dl_binaries/bin:/usr/local/cuda/bin:/usr/libexec/gcc/x86_64-amazon-linux/4.8.5:/home/ec2-user/anaconda3/envs/tensorflow_p36/bin:/home/ec2-user/anaconda3/condabin:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/home/ec2-user/anaconda3/condabin:/home/ec2-user/.dl_binaries/bin:/usr/local/cuda/bin:/usr/libexec/gcc/x86_64-amazon-linux/4.8.5:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/opt/aws/bin:/home/ec2-user/anaconda3/envs/JupyterSystemEnv/bin\n"
     ]
    }
   ],
   "source": [
    "# add NPM to the path so we can assemble the web frontend from our notebook code\n",
    "\n",
    "from os import environ\n",
    "\n",
    "npm_path = ':/home/ec2-user/anaconda3/envs/JupyterSystemEnv/bin'\n",
    "\n",
    "if npm_path not in environ['PATH']:\n",
    "    ADD_NPM_PATH = environ['PATH']\n",
    "    ADD_NPM_PATH = ADD_NPM_PATH + npm_path\n",
    "else:\n",
    "    ADD_NPM_PATH = environ['PATH']\n",
    "    \n",
    "%set_env PATH=$ADD_NPM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/amazon-sagemaker-visual-search/frontend\n",
      "\u001b[K\u001b[?25h        \u001b[27m\u001b[90m......\u001b[0m] - postinstall:ansi-colors: \u001b[32minfo\u001b[0m \u001b[35mlifecycle\u001b[0m ansi-colors@3.2.\u001b[0m\u001b[K\u001b[K\n",
      "> core-js@2.6.11 postinstall /home/ec2-user/SageMaker/amazon-sagemaker-visual-search/frontend/node_modules/babel-runtime/node_modules/core-js\n",
      "> node -e \"try{require('./postinstall')}catch(e){}\"\n",
      "\n",
      "\u001b[96mThank you for using core-js (\u001b[94m https://github.com/zloirock/core-js \u001b[96m) for polyfilling JavaScript standard library!\u001b[0m\n",
      "\n",
      "\u001b[96mThe project needs your help! Please consider supporting of core-js on Open Collective or Patreon: \u001b[0m\n",
      "\u001b[96m>\u001b[94m https://opencollective.com/core-js \u001b[0m\n",
      "\u001b[96m>\u001b[94m https://www.patreon.com/zloirock \u001b[0m\n",
      "\n",
      "\u001b[96mAlso, the author of core-js (\u001b[94m https://github.com/zloirock \u001b[96m) is looking for a good job -)\u001b[0m\n",
      "\n",
      "\n",
      "> core-js@3.6.5 postinstall /home/ec2-user/SageMaker/amazon-sagemaker-visual-search/frontend/node_modules/core-js\n",
      "> node -e \"try{require('./postinstall')}catch(e){}\"\n",
      "\n",
      "\n",
      "> core-js-pure@3.6.5 postinstall /home/ec2-user/SageMaker/amazon-sagemaker-visual-search/frontend/node_modules/core-js-pure\n",
      "> node -e \"try{require('./postinstall')}catch(e){}\"\n",
      "\n",
      "\u001b[K\u001b[?25h7m            \u001b[27m\u001b[90m......\u001b[0m] / postinstall:ext: \u001b[32minfo\u001b[0m \u001b[35mlifecycle\u001b[0m ext@1.4.0~postinstall: e\u001b[0m\u001b[K\n",
      "> fast-xml-parser@3.17.4 postinstall /home/ec2-user/SageMaker/amazon-sagemaker-visual-search/frontend/node_modules/fast-xml-parser\n",
      "> node tasks/postinstall.js || exit 0\n",
      "\n",
      "\u001b[96m\u001b[1mLove fast-xml-parser? Check \u001b[32mhttps://amitkumargupta.work \u001b[96m\u001b[1mfor more projects and contribution.\u001b[0m\n",
      "\n",
      "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35moptional\u001b[0m SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.13 (node_modules/webpack-dev-server/node_modules/fsevents):\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.13: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"})\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35moptional\u001b[0m SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.13 (node_modules/watchpack-chokidar2/node_modules/fsevents):\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.13: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"})\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35moptional\u001b[0m SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.13 (node_modules/jest-haste-map/node_modules/fsevents):\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.13: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"})\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35moptional\u001b[0m SKIPPING OPTIONAL DEPENDENCY: fsevents@2.1.2 (node_modules/fsevents):\n",
      "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.1.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"})\n",
      "\u001b[0m\n",
      "added 1823 packages from 1205 contributors and audited 1828 packages in 49.676s\n",
      "\n",
      "65 packages are looking for funding\n",
      "  run `npm fund` for details\n",
      "\n",
      "found \u001b[91m253\u001b[0m vulnerabilities (3 \u001b[1mlow\u001b[0m, 143 \u001b[93mmoderate\u001b[0m, 102 \u001b[91mhigh\u001b[0m, 5 \u001b[95mcritical\u001b[0m)\n",
      "  run `npm audit fix` to fix them, or `npm audit` for details\n",
      "\u001b[K\u001b[?25h7m             \u001b[27m\u001b[90m.....\u001b[0m] \\ prepare:frontend: \u001b[7msill\u001b[0m \u001b[35minstall\u001b[0m printInstalled\u001b[0m\u001b[K"
     ]
    }
   ],
   "source": [
    "%cd ./frontend/\n",
    "\n",
    "!npm install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> frontend@0.1.0 build /home/ec2-user/SageMaker/amazon-sagemaker-visual-search/frontend\n",
      "> react-scripts build\n",
      "\n",
      "Creating an optimized production build...\n",
      "Browserslist: caniuse-lite is outdated. Please run:\n",
      "npx browserslist@latest --update-db\n",
      "\u001b[32mCompiled successfully.\u001b[39m\n",
      "\u001b[32m\u001b[39m\n",
      "File sizes after gzip:\n",
      "\n",
      "  194.79 KB  \u001b[2mbuild/static/js/\u001b[22m\u001b[36m2.385ac3cf.chunk.js\u001b[39m\n",
      "  4.5 KB     \u001b[2mbuild/static/css/\u001b[22m\u001b[36m2.4a00a097.chunk.css\u001b[39m\n",
      "  2.37 KB    \u001b[2mbuild/static/js/\u001b[22m\u001b[36mmain.f333f224.chunk.js\u001b[39m\n",
      "  770 B      \u001b[2mbuild/static/js/\u001b[22m\u001b[36mruntime-main.fc75219d.js\u001b[39m\n",
      "  547 B      \u001b[2mbuild/static/css/\u001b[22m\u001b[36mmain.5f361e03.chunk.css\u001b[39m\n",
      "\n",
      "The project was built assuming it is hosted at \u001b[32m/\u001b[39m.\n",
      "You can control this with the \u001b[32mhomepage\u001b[39m field in your \u001b[36mpackage.json\u001b[39m.\n",
      "\n",
      "The \u001b[36mbuild\u001b[39m folder is ready to be deployed.\n",
      "You may serve it with a static server:\n",
      "\n",
      "  \u001b[36mnpm\u001b[39m install -g serve\n",
      "  \u001b[36mserve\u001b[39m -s build\n",
      "\n",
      "Find out more about deployment here:\n",
      "\n",
      "  \u001b[33mbit.ly/CRA-deploy\u001b[39m\n",
      "\n",
      "\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[33m   \u001b[39m\n",
      "   \u001b[33m\u001b[39m                                                               \u001b[33m\u001b[39m\n",
      "   \u001b[33m\u001b[39m      New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m6.14.15\u001b[39m  \u001b[32m8.6.0\u001b[39m      \u001b[33m\u001b[39m\n",
      "   \u001b[33m\u001b[39m   \u001b[33mChangelog:\u001b[39m \u001b[36mhttps://github.com/npm/cli/releases/tag/v8.6.0\u001b[39m   \u001b[33m\u001b[39m\n",
      "   \u001b[33m\u001b[39m               Run \u001b[32mnpm install -g npm\u001b[39m to update!               \u001b[33m\u001b[39m\n",
      "   \u001b[33m\u001b[39m                                                               \u001b[33m\u001b[39m\n",
      "\u001b[33m   \u001b[39m\n",
      "\u001b[33m\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!npm run-script build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: build/asset-manifest.json to s3://vis-search-s3buckethosting-1st2s59fkit8r/asset-manifest.json\n",
      "upload: build/robots.txt to s3://vis-search-s3buckethosting-1st2s59fkit8r/robots.txt\n",
      "upload: build/logo192.png to s3://vis-search-s3buckethosting-1st2s59fkit8r/logo192.png\n",
      "upload: build/index.html to s3://vis-search-s3buckethosting-1st2s59fkit8r/index.html\n",
      "upload: build/static/js/main.f333f224.chunk.js to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/js/main.f333f224.chunk.js\n",
      "upload: build/static/css/2.4a00a097.chunk.css.map to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/css/2.4a00a097.chunk.css.map\n",
      "upload: build/service-worker.js to s3://vis-search-s3buckethosting-1st2s59fkit8r/service-worker.js\n",
      "upload: build/static/css/2.4a00a097.chunk.css to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/css/2.4a00a097.chunk.css\n",
      "upload: build/static/js/runtime-main.fc75219d.js to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/js/runtime-main.fc75219d.js\n",
      "upload: build/static/css/main.5f361e03.chunk.css to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/css/main.5f361e03.chunk.css\n",
      "upload: build/static/css/main.5f361e03.chunk.css.map to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/css/main.5f361e03.chunk.css.map\n",
      "upload: build/static/js/runtime-main.fc75219d.js.map to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/js/runtime-main.fc75219d.js.map\n",
      "upload: build/static/js/2.385ac3cf.chunk.js.LICENSE.txt to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/js/2.385ac3cf.chunk.js.LICENSE.txt\n",
      "upload: build/static/media/UploadIcon.1cedb6e9.svg to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/UploadIcon.1cedb6e9.svg\n",
      "upload: build/static/js/main.f333f224.chunk.js.map to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/js/main.f333f224.chunk.js.map\n",
      "upload: build/static/media/roboto-latin-100.7370c367.woff2 to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-100.7370c367.woff2\n",
      "upload: build/static/media/roboto-latin-100italic.f8b1df51.woff2 to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-100italic.f8b1df51.woff2\n",
      "upload: build/static/media/roboto-latin-100.5cb7edfc.woff to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-100.5cb7edfc.woff\n",
      "upload: build/static/media/roboto-latin-300.ef7c6637.woff2 to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-300.ef7c6637.woff2\n",
      "upload: build/static/media/roboto-latin-100italic.f9e8e590.woff to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-100italic.f9e8e590.woff\n",
      "upload: build/static/media/roboto-latin-300.b00849e0.woff to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-300.b00849e0.woff\n",
      "upload: build/static/media/header.c8606335.jpg to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/header.c8606335.jpg\n",
      "upload: build/static/js/2.385ac3cf.chunk.js to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/js/2.385ac3cf.chunk.js\n",
      "upload: build/static/media/roboto-latin-300italic.4df32891.woff to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-300italic.4df32891.woff\n",
      "upload: build/static/media/roboto-latin-400italic.fe65b833.woff to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-400italic.fe65b833.woff\n",
      "upload: build/static/media/roboto-latin-300italic.14286f3b.woff2 to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-300italic.14286f3b.woff2\n",
      "upload: build/static/media/roboto-latin-400.60fa3c06.woff to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-400.60fa3c06.woff\n",
      "upload: build/static/media/roboto-latin-400.479970ff.woff2 to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-400.479970ff.woff2\n",
      "upload: build/static/media/roboto-latin-500.87284894.woff to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-500.87284894.woff\n",
      "upload: build/precache-manifest.1faf04ff74e8c89fa7aa5cc465004f5e.js to s3://vis-search-s3buckethosting-1st2s59fkit8r/precache-manifest.1faf04ff74e8c89fa7aa5cc465004f5e.js\n",
      "upload: build/static/media/roboto-latin-500.020c97dc.woff2 to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-500.020c97dc.woff2\n",
      "upload: build/static/media/roboto-latin-400italic.51521a2a.woff2 to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-400italic.51521a2a.woff2\n",
      "upload: build/static/media/roboto-latin-700italic.da0e7178.woff2 to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-700italic.da0e7178.woff2\n",
      "upload: build/static/media/roboto-latin-900italic.28f91510.woff to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-900italic.28f91510.woff\n",
      "upload: build/static/media/roboto-latin-700italic.81f57861.woff to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-700italic.81f57861.woff\n",
      "upload: build/static/media/roboto-latin-700.adcde98f.woff to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-700.adcde98f.woff\n",
      "upload: build/static/media/roboto-latin-700.2735a3a6.woff2 to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-700.2735a3a6.woff2\n",
      "upload: build/static/media/roboto-latin-900.bb1e4dc6.woff to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-900.bb1e4dc6.woff\n",
      "upload: build/favicon.ico to s3://vis-search-s3buckethosting-1st2s59fkit8r/favicon.ico\n",
      "upload: build/static/media/roboto-latin-500italic.db4a2a23.woff2 to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-500italic.db4a2a23.woff2\n",
      "upload: build/static/js/2.385ac3cf.chunk.js.map to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/js/2.385ac3cf.chunk.js.map\n",
      "upload: build/manifest.json to s3://vis-search-s3buckethosting-1st2s59fkit8r/manifest.json\n",
      "upload: build/static/media/roboto-latin-900.9b3766ef.woff2 to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-900.9b3766ef.woff2\n",
      "upload: build/logo512.png to s3://vis-search-s3buckethosting-1st2s59fkit8r/logo512.png\n",
      "upload: build/static/media/roboto-latin-900italic.ebf6d164.woff2 to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-900italic.ebf6d164.woff2\n",
      "upload: build/static/media/roboto-latin-500italic.288ad9c6.woff to s3://vis-search-s3buckethosting-1st2s59fkit8r/static/media/roboto-latin-500italic.288ad9c6.woff\n"
     ]
    }
   ],
   "source": [
    "hosting_bucket = f\"s3://{outputs['s3BucketHostingBucketName']}\"\n",
    "\n",
    "!aws s3 sync ./build/ $hosting_bucket --acl public-read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Browse your frontend service, and upload an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click the URL below:\n",
      "\n",
      "https://vis-search-s3buckethosting-1st2s59fkit8r.s3.amazonaws.com/index.html\n"
     ]
    }
   ],
   "source": [
    "print('Click the URL below:\\n')\n",
    "print(outputs['S3BucketSecureURL'] + '/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the following page:\n",
    "\n",
    "![Website](pi3small.png)\n",
    "\n",
    "On the website, try pasting the following URL in the URL text field.\n",
    "\n",
    "`https://i4.ztat.net/large/VE/12/1C/14/8K/12/VE121C148-K12@10.jpg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "We have used pretrained Resnet50 model which is trained on Imagenet dataset. Now based on your use-case you can fine tune any pre-trained models, such as VGG, Inception, and MobileNet with your own dataset and host the model in Amazon SageMaker.\n",
    "\n",
    "You can also use Amazon SageMaker Batch transform job to have a bulk feaures extracted from your stored S3 images and then you can use AWS Glue to import that data into Elasticeearch domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Make sure that you stop the notebook instance, delete the Amazon SageMaker endpoint and delete the Elasticsearch domain to prevent any additional charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty S3 Contents\n",
    "training_bucket_resource = s3_resource.Bucket(bucket)\n",
    "training_bucket_resource.objects.all().delete()\n",
    "\n",
    "hosting_bucket_resource = s3_resource.Bucket(outputs['s3BucketHostingBucketName'])\n",
    "hosting_bucket_resource.objects.all().delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
